---
output:
  pdf_document: default
  html_document: default
---
<!--- Below are global settings for knitr. You can override any of them by adding the changes to individual chunks --->

```{r global_options, include=FALSE}
knitr::opts_chunk$set(error=TRUE,        # Keep compiling upon error
                      collapse=FALSE,    # collapse by default
                      echo=TRUE,         # echo code by default
                      comment = "#>",    # change comment character
                      fig.width = 5,     # set figure width
                      fig.align = "center",# set figure position
                      out.width = "49%", # set width of displayed images
                      warning=FALSE,     # do not show R warnings
                      message=FALSE)     # do not show R messages
```

<!--- Solution Region --->
<style>
#solution {
  background-color: #8FBC8F;
  border-style: solid;
  border-color: blue;
  padding: .5em;
  margin: 20px
}
</style>



**SYS 6018 | Fall 2019 | University of Virginia **

*******************************************

<!--- Load Required R packages here --->
```{r, include=FALSE}
#install.packages("mixtools")
#install.packages("mclust")
library(tidyverse)
library(mixtools)
library(mclust)
library(gridExtra)
library(grid)
digits <- function(x, k=2) format(round(x, k), nsmall=k)
```



### Problem 4.1: Clustering 


The data for this problem are here: <https://raw.githubusercontent.com/mdporter/SYS6018/master/data/clusthw.csv>
```{r}
url <- "https://raw.githubusercontent.com/mdporter/SYS6018/master/data/clusthw.csv"
destfile <- "4_1Data.csv"
curl::curl_download(url, destfile)
Cluster_Data <- read.csv(destfile) 
```


1. Run Hierarchical clustering, using Euclidean distance, and two linkage methods (of your choice). Show the resulting dendrograms. 

<div id="solution">
```{r}
d <- dist(Cluster_Data, method = "euclidean")
hc <- hclust(d, method = "average")
hc_single <- hclust(d, method = "single")
hc_complete <- hclust(d, method = "complete")
(plot(as.dendrogram(hc)))
(plot(as.dendrogram(hc_single)))
(plot(as.dendrogram(hc_complete)))

```

</div>


2. Estimate $K$ for one of the linkage methods from part a. Explain why you chose that value of $K$. 

<div id="solution">
```{r}
n = length(hc_single$height)     
plot(n:1, hc_single$height, type='o', xlab="K", ylab="height", las=1, 
     xlim=c(1, 50))
points(12, hc_single$height[n-11], col="red", pch=19) 
yhat <- cutree(hc_single, h = .06)

k <- length(unique(yhat))

k

```

I chose K = 43 because the dissimilarity between clusters seems to spike around h = .6 based on visual analysis of the dendrogram. Cutting the tree at h = .6 yields 43 clusters.  
</div>

3. Show a scatterplot of the data using colors to denote the $K$ clusters. Based on a visual analysis, does it appear that $K$ means was successful? Explain any changes to the clustering that you think should be made. 

<div id="solution">
```{r}
hc_single$order
clusters <- data.frame(index = hc_single$order, cluster = yhat) %>% arrange(index)
Cluster_Data$cluster <- clusters$cluster

ggplot(Cluster_Data, mapping = aes(x = x, y = y, col = cluster)) +
  
  geom_point()

```
43 clusters appear to be quite excessive; the number should probably be reduced to 3 or 4. 
</div>


4. Run $K$-means for a sequence of $K$ values. Plot the sum of squared errors (SSE) as a function of $K$. 

<div id="solution">
```{r}

K <- tibble(k = seq(1,43))

SSE <- vector()

for (k in seq(1,43)) {
  
  km.out = kmeans(Cluster_Data,k,nstart = 20)
  SSE <- c(SSE, km.out$tot.withinss)
  
}

K$SSE <- SSE

K
  
plot(K,pch = 20, cex = 1, main = "K vs. SSE")
```

</div>


5. Estimate $K$. Explain why you chose that value of $K$. 

<div id="solution">
I estimate K = 7 because the "K vs. SSE" plot above indicates that further partitions do not significantly reduce the total within-cluster SSEs. 
</div>


6. Show a scatterplot of the data using colors to denote the $K$ clusters. Based on a visual analysis, does it appear that $K$ means was successful? Explain any changes to the clustering that you think should be made. 

<div id="solution">
```{r}
km.out = kmeans(Cluster_Data,7,nstart = 20)
plot(x = Cluster_Data$x, y = Cluster_Data$y, col=(km.out$cluster + 1), main = "K-Means Results for K = 7", pch = 20, cex = 1.5)
```
K means with K = 7 was somewhat successful -- it correctly segmented the small, dense cluster -- but seems to have oversegmented the data in some places. The clusters in lower end of x space overlap. There are a few outliers in x space which may be skewing results. I think the true clustering is probably closer to K = 3 or K = 4, but K means fails to accurately partition the data at that level (see additional graph below)

```{r}
km.out = kmeans(Cluster_Data,4,nstart = 20)
plot(x = Cluster_Data$x, y = Cluster_Data$y, col=(km.out$cluster + 1), main = "K-Means Results for K = 4", pch = 20, cex = 1.5)
```


</div>



### Problem 4.2: Activity Recognition Challenge

A current engineering challenge is to identify/classify human activity (e.g., walking, in car, on bike, eating, smoking, falling) from smartphones and other wearable devices. 
More specifically, the embedded sensors (e.g., accelerometers and gyroscopes) produce a time series of position, velocity, and acceleration measurements. These time series are then processed to produce a set of *features* that can be used for activity recognition. We will use a subset of such features to cluster an activity dataset. The dataset `activity.csv` contains six features that correspond to $K$ human activities. Your challenge is to cluster these data. 

You can use any clustering method you like. You are free to transform or pre-process the data. 

This will be a contest, so you will submit your cluster scores and we will evaluate how closely your clusters match the true activities. The reported clusters will be evaluated by the [*Adjusted Rand index (ARI)*](https://en.wikipedia.org/wiki/Rand_index).
You will receive credit for a proper submission and code; the top five scores will receive 2 bonus points. 


```{r, echo=FALSE}
data.dir = 'https://raw.githubusercontent.com/mdporter/SYS6018/master/data/activity.csv'
destfile <- '4_2.csv'
curl::curl_download(data.dir, destfile)
tracking <- read_csv('4_2.csv')

```

`activity.csv`: <`r file.path(data.dir, 'activity.csv')`>


a. Submit cluster labels for all observations. Your file should be a .txt with no header and no row numbers (i.e., 5000 rows, 1 column). 
Name the file `lastname_firstname.txt`. We will use automated evaluation, so the format must be exact. 

```{r, eval=FALSE}
#-- Example of making submission data. 
# est.label is a vector of labels (length 5000)
#write.table(est.l, file="save.dir/lastname_firstname.txt", 
         # row.names=FALSE, col.names=NA)
```


b. Show your code. 

<div id="solution">
```{r}
BIC <- mclustBIC(tracking)
BIC_2 <- mclustBIC(dplyr::select(tracking, X1, X2, X3, X4, X5))
plot(BIC)
plot(BIC_2)
mod <- Mclust(tracking, x = BIC)
mod_2 <- Mclust(tracking, x = BIC_2)
mod_3 <- Mclust(tracking, G = 11)
?Mclust()
summary(mod, parameters = TRUE)
summary(mod_2, parameters = TRUE)
summary(mod_3, parameters = TRUE)
plot(mod_2, what = "classification")
```


```{r}
classes <- mod_2$classification
write.table(classes, file="Stein_Rebecca.txt", col.names= FALSE, row.names = FALSE)
```

</div>



### Problem 4.3: Poisson Mixture Model


The pmf of a Poisson random variable is:
\begin{align*}
f_k(x; \lambda_k) = \frac{\lambda_k^x e^{-\lambda_k}}{x!}
\end{align*}

A two-component Poisson mixture model can be written:
\begin{align*}
f(x; \theta) = \pi \frac{\lambda_1^x e^{-\lambda_1}}{x!} + (1-\pi) \frac{\lambda_2^x e^{-\lambda_2}}{x!}
\end{align*}



a. What are the parameters of the model? 

<div id="solution">
$\theta = (\lambda_1, \lambda_2, \pi_1, \pi_2)$
</div>


b. Write down the log-likelihood for $n$ observations ($x_1, x_2, \ldots, x_n$). 

<div id="solution">
$log(L(X : \theta) = \sum_1^nlog([(\pi_1\lambda_1 ^ xexp(-\lambda_1) + ((1-\pi_1)\lambda_2 ^ xexp(-\lambda_2)]$ $/ x!)$
</div>


c. Suppose we have an initial value of the parameters. Write down the equation for updating the *responsibilities*. 

<div id="solution">
$r(xk) = \lambda_k ^ xexp(-\lambda_k) / \sum_k\lambda_k ^ xexp(-\lambda_k)$
</div>



d. Suppose we have responsibilities, $r_{ik}$ for all $i=1, 2, \ldots, n$ and $k=1,2$. Write down the equations for updating the parameters. 

<div id="solution">
$n_k = \sum_i rsubik$
$\pi_k = n_k / n$
$\lambda_k = argmax(log(L(X : \lambda_k)$

</div>



e. Fit a two-component Poisson mixture model, report the estimated parameter values, and show a plot of the estimated mixture pmf for the following data:

```{r, echo=TRUE}
#-- Run this code to generate the data
set.seed(123)             # set seed for reproducibility
n = 200                   # sample size
z = sample(1:2, size=n, replace=TRUE, prob=c(.25, .75)) # sample the latent class
theta = c(8, 16)          # true parameters
y = ifelse(z==1, rpois(n, lambda=theta[1]), rpois(n, lambda=theta[2]))
```


<div style="background-color:lightgrey; display: block; border-color: black; padding:1em">

Note: The function `poisregmixEM()` in the R package `mixtools` is designed to estimate a mixture of *Poisson regression* models. We can still use this function for our problem of density estimation if it is recast as an intercept-only regression. To do so, set the $x$ argument (predictors) to `x = rep(1, length(y))` and `addintercept = FALSE`. 

Look carefully at the output from this model. The `beta` values (regression coefficients) are on the log scale.

</div>


<div id="solution">
```{r}
twocomppois <- poisregmixEM(y = y, x = rep(1,length(y)), k = 2, addintercept = FALSE)

plot.mixEM(twocomppois)
```

</div>




